---
title: "LangCon2019 후기"
date: 2019-02-25 00:00:01 
categories: seminar
tags:
  - 자연어처리
  - nlp
  - 한글
---

# Language conference 2019: 자연어 처리하기 좋은 날입니다 

[LangCon2019 : 자연어 처리하기 좋은 날입니다!](https://festa.io/events/198)이 2019년 2월 16일 토요일에 마이크로소프트 광화문오피스에서 열렸습니다. 작년에 [hands on:사람이 챗봇을 만듭니다!](https://www.onoffmix.com/event/124842)에 참석했을 때도 참석하여 많은 배움과 자극을 얻었기 때문에 이번에도 참석하였습니다. 자연어 처리에 관심은 많은 반면에 누군가 만들어주신 걸 사용하기만 할 뿐 제가 구현하지는 못하고 있기 때문에 발표자분들의 열정과 노력의 결과물들이 값지고 멋지게 느껴졌습니다. 다만 한글이 아닌 프로젝트와 발표는 아쉬웠습니다. 영어와 한국어의 문법의 차이가 있기 존재하기 때문에 한글 문장으로 그대로 적용해보기는 무리가 있다고 생각해 크게 의미있게 다가오지는 않았지만 이제 막 시작하는 분들에게는 괜찮은 내용이었다고 생각합니다. 관심있는 분들을 위해 세미나 내용을 키워드를 중심으로 정리해보겠습니다. 


## 키노트1: 데이터 사이언티스트가 인공지능 서비스(chatbot)를 오픈하는 방법

**챗봇 서비스**: 발표자분은 LG에서 근무하시면서 챗봇을 개발하여 콜센터 상담비율을 많이 줄였다고 합니다. 자연어처리 및 백엔드는 자체 개발하여 사용하고 있고 aws를 사용하고 있다고 합니다. 아직 챗봇은 인공지능이란 기술의 문제도 있지만, 막연한 기대 때문에 이게 뭐야 이런 이야기가 나올 수 있다고 합니다. 인공지능 서비스에 대해 사람마다 다른 기대치를 가지기 때문에 그 기대치를 충족하기 어렵다고 합니다. 특히 오타 보정, 자기소개, 날씨 질문과 같은 주 목적이 아닌 어려운 것에 관심을 가져 굉장히 힘들다고 합니다. 그래서 목적을 잘정리하여 precision과 recall을 정하여 기본적인 내용을 커버하는 것이 중요하다고 합니다. 또 강조하셨던 부분이 포지션을 잘 가져가면 시장에서도 큰 기회를 가질 수 있다고 합니다. 문헌정보학, 언어학과 출신 분들이 데이터 모델링 및 기획 분야에서 활약하고 있다고 합니다.

**챗봇 방향성**: 챗봇은 홈페이지와 차이가 있다고 합니다. 홈페이지는 하이라키컬하게 인터페이스를 정리할 수 있지만 챗봇은 컨텐츠를 바로 보여주기 때문에 큰 기회를 갖고 있는 시장입니다. 광고를 삽입하거나 대화를 통한 추가 정보를 제공할 수 있습니다. 챗봇은 인터페이스이기 때문에 컨텐츠/연계 시스템이 중요합니다. 핵심은 자동화에 있습니다. intent와 entity들이 자동화를 하기 위한 파라미터가 될 수 있습니다. FAQ 350개의 entity를 생성할 바에 검색이 더 낫습니다. 챗봇은 goal based 챗봇과 non goal based 챗봇으로 구분합니다. goal based 챗봇중 cs는 목적을 명확히 하여 화를 돋구면 안됩니다. ^^ , marketing 챗봇은 온보딩 메시지를 잘 정의하는 것이 중요합니다.

**NLU**: 인덴트를 명확히 하면서 학습셋을 얼마나 넣어 학습해야하는지 가늠이 안됩니다. auto ml 같은 경우는 아직은 모르겠습니다. 사람하고 대화하는건 다양성이 높고 턴마다 대화주제가 다르기 때문에 아직은 어렵다고 합니다. 해당 도메인에서 쓰는 약어들은 분석가 또는 현업 담당자들이 한다고 합니다. 주제 분류를 할때 토픽모델링을 사용하되 사람이 같이 봐야한다고 합니다. 자주 얘기되는 모델에 대해서는 재학습 또는 재처리를 해야한다고 불용어는 tf를 사용해서 처리하는게 좋다고 합니다. 복합 단어는 bi-gram을 사용하면 모든 객체에 대한 단어들은 인식한다고 합니다.


**데이터 분석**: 정량적, 텍스트로 분리 하여 분석 가능합니다. goal based는 정의한 문장까지 가야지 성공 가지 못하면 실패입니다. 운영 데이터 분석은 별로 관심들이 없어 안타깝다고 ... 자연어 분석의 어려움이 있고, 목적에 따라 다르기 때문에 재학습 전략이 필요하고 인덴트 개수가 늘어날 수록 추론에 대한 복잡도가 늘어난다고 합니다. 자연어 처리에 대한 계층적 구조를 어떻게 정의하는게 중요하다고 합니다.

실제 챗봇 서비스를 어떻게 개발하고 운영하는지 실제 경험 기반의 가치 있는 이야기를 들을 수 있어 굉장히 좋았습니다.


## 키노트2: khaii - 딥러닝 기반 형태소 분석기

**형태소와 어절**: 영어의 경우 운문을 단어 또는 토큰화할 수 있지만 한국어의 경우 어절단위로 토큰화 했을 때 변형되는 형태 때문에 문제점이 발생합니다. 형태소 단위로 분석 했을 때 의미가 있는 실질 형태소롤 사용할 수 있씁니다. 형태소 분석의 

**어려움**: 두가지 중의성, 어절안에서 형태소 분리할 때, 품사 중의성이 있습니다. 띄어 쓰기의 경우 사용자의 빈번한 실수로 어렵습니다. 원형 복원의 경우 '쉬워'-> *쉽다+어*가 되지만 '비워'의 경우 *비우다+어*가 되는 경우가 있습니다. 복합명사의 경우 붙여서또 띄어 써도 맞다고 합니다. 

**세종코퍼스**: 가장 큰 코퍼스이며 가장 많은 연구가 진행되어 사실상 표준이라고 합니다. 하지만 오류도 많고 저작권의 이유로 수정본 공개는 불가능하다고 합니다. 원문 자체에 저작권이 있어 카카오에서는 [문종 프로젝트](https://github.com/kakao/khaiii/tree/master/munjong)라 불리는 자동으로 코퍼스를 수정하는 스크립트를 공개하였다고 합니다. 

**방법**: bi-lstm with attention은 순차 계산 및 병렬 처리가 불가능하여 음절기반 방법의 cnn모델을 사용하여 가상 음절을 구성해서 입력에 따른 출력을 정의하였다고 합니다. 2,3,4,5gram의 4가지 모델을 사용하였다고 합니다. 정확도와 속도에는 윈도우, 임베딩 사이즈가 가장 큰 영향을 미치고 정확도와 재현률의 조화 평균인 f-score에 따라 윈도우 크기 3,4 임베딩 사이즈는 클수록 좋지만 150부터는 더디게 올라간다고 합니다. 사용자 사전은 단일 어절에 대한 분석, 오분석 패치는 여러 어절에 걸쳐 충분한 문맥과 함께 분석한다고 합니다. 많은 사용 및 참여를 원한다고 합니다. [khaiii](https://github.com/kakao/khaiii)


## 키노트3: 스마트 스피커에서의 음악 재생 발화 오류 교정

카카오 스마트 스피커 관련 업무를 맡고 계시다는 저자는 많은 기업에서 스마트 스피커를 왜 개발하고 있는지 설명해주셨습니다. 스마트 스피커의ㅣ 시장을 선점하게 된다면 승자독식 가능하다고 합니다. 스카트 스피커를 바탕으로 음악, 영상, 쇼핑까지 모든 것을 해결하고 이용할 수 있어서 파급효과가 강력하다고 합니다. 스마트 스피커의 작동방식은 *음성인식->자연어이해->검색->자연어생성->음성합성*의 절차로 이뤄집니다.
오타교정은 잘못된 질의로부터 올바른 질의로 교정하는 기술로 *오타판별 -> 후보검색 -> 후보 검증*의 단계를 거치면 noise channel model을 사용한다고 합니다. 개발 과정이 특이했는데 토이 프로젝트로 시작하여 서비스 개발 및 적용까지 이뤄지게 된 개발자라면 누구나 꿈꾸는 취미로 만든 프로젝트가 실제 회사내의 프로젝트로 이어지는 바람직한 현장이었습니다. 후보 생성 모델은 유사도가 높은 후보를 출력하는 모델로 유사 후보는 최대 힙에 저장하고 정답후보 사전은 편집거리가 고려된 트라이에 저장하여 사용한다고 합니다. 현업에서 실제로 사용중인 프로젝트가 개인의 토이 프로젝트에서 시작했다고 하니 놀랍고 부럽습니다. 서비스 기업의 힘일 것 같다는 생각이 듭니다.


### 뉴스를 이용한 주식시장 예측

부동산 관련 기업에서 일하고 계시는 저자는 텍스트를 시계열모델에 반영하면 어떤 결과를 얻을 수 있지 않을까란 생각에서 시작하셨다고 합니다. kaggle에서 데이터를 수집하셨고 뉴스데이터는 reddit에서 주식데이터는 다우존스산업평균지수를 사용하셨다고 합니다. 데이터 탐색 부터 데이터 전처리, 모델링, 분석 및 결과, 개선방향까지 데이터 분석의 기본적인 흐름을 차근차근 알려주셔서 좋았습니다. 한국 데이터였으면 더 좋았을 것 같다는 생각이 듭니다. 흥미로운 질문으로는 실무에서는 aws나 azure클라우드의 모델을 사용하고 있고 부동산 예측에서 의미있는 변수가 책에 나오는 키워드를 중심으로 대입해보면 의미있는 변수를 찾을 수 있다고 합니다. 시계열 모델 말고 부스팅 모델을 사용하는데 증명되지 않았지만 결과가 좋게 나오므로 사용하고 있다고 합니다. 한글을 영문으로 번역하여 사용하는 것도 시도해볼만하다고 합니다.


### 문장 속 단어

언어학도이신 발표자분은 자연어처리에 대한 전반적인 개념을 언어학적인 관점에서 설명해주셨습니다. 자연어 처리를 잘한다는 것은 형태소 분석을 잘한다는 것이고 자연어의 환계는 차원의 한계입니다. 우리가 인지하는 세상은 3차원+시간의 흐름이 있는데 반해 언어는 선형적(linear)하여 이 차원의 격차를 어떻게 메울지 중요하다고 합니다. 그 전략 중 하나가 적당히 말하고 적당히 알아 듣는 것 ->화용론(pragmatics), 그리고 내 머릿속 세상의 등장인물들과 그 동작, 상태에 이름표를 달아 표현 하는 것 -> 통사론(syntax), 등장 인물과 동작, 상태를 개념화해서 표현하는것 -> 형태론(morphology)가 있다고 합니다. 형태소 분석의 문제 중 분절의 문제가 있는데 두 요소가 독립적으로 의미가 파악이 된다면 나누고 묶어야 정확한 의미가 나온다면 나누지 않는 것을 원칙으로 한다고 합니다. 속하신 랩실에서 형태소 분석기를 연구하고 계신거 같은데 오픈소스로 열어주셔서 자연어 처리 생태계가 더욱 풍성해졌으면 하는 바람입니다.


### 너의목소리가들려(광화문1번가 분석)

인문학도이신 발표자 분들은 광화문 1번가를 분석하셨습니다. 강의에서 배우신대로 연구 배경, 연구 방법, 연구결과를 흐름에 맞게 잘 설명해주셨습니다. 데이터를 크롤링하고 긍정,부정 단어를 정의하고 규칙을 만들어서 데이터를 분석하는 시도들이 멋졌습니다. 
		
### 트랜스퍼 러닝과 텍스트 문서의 분류

변호사이신 발표자분은 트랜스퍼 러닝을 사용한 경험을 중심으로 이야기 해주셨습니다. 트랜스퍼 러닝은 다른 분야의 학습 모델을 가져와서 유사 분야에서 적용하는 것으로 일부 또는 전체를 교체 가능하다고 합니다. 이는 성능의 향상 및 모델 개발/학습 시간을 단축시키는 효과가 있습니다. 영상 분야에서 트랜스퍼 러닝은 활발하여 자연어 처리에서 워드 임베딩을 적용하여 이전 보다 더 나은 결과를 보여 주었다는 것을 알았습니다. 학습에 필요한 데이터가 부족하거나 성능향상이 필요할 때 트랜스퍼러닝 고려해볼 수 있다고 하고 최근 논문에서 이미지넷 데이터를 학습한 모델을 바탕으로 마지막 레이어만 제대로 학습시켜서 사용한경우 망막의 당뇨병여부 판단 가능한 모델이 있다는 연구가 흥미로웠습니다.

		
### 한국어 의존성 분석 이론 및 동향

발표자분은 컴파일 관련 연구를 하다가 의존성 분석과 파싱 알고리즘이 관련이 있다는 것을 알고 취미로 의존성 분석을 하고 계씨다고 합니다. 의존성 분석이란 문장의 문법적 구조를 파악하여 각 단어별 관계성을 찾는 방법이고 의존성 분석은 문장의 구조적 모호성을 해결하기 위해 사용한다고 합니다. 하지만 시간이 많이 걸리기 때문에 실시간 프로그램에서 적용한 걸 찾기는 어렵다고 합니다. 의존성 분석을 위한 알고리즘에는 비결정적 의존 구문 분석, 모든 의존 트리 중에서 가장 높은 점수의 의존트리를 선택하는 방법, 결정적 의존 구문 분석, 일종의 탐욕적 알고리즘에 기반한 방법으로 지역적 학습모델을 사용 전이기반 의존 구문 분석이 있다고 합니다.


정말 뜻깊고 유익한 시간이었습니다. 발표자분들의 경험과 생각을 직접 들을 수 있다는 점에서 매우 좋았습니다. 최근 생성 음악및 음악 시각화에 조금 더 관심을 두고 있어 자연어 처리에 신경을 많이 못쓰고 있지만 언젠가 자연어 처리 오픈소스 프로젝트에 컨트리뷰터가 되는 것이 목표이지만 목구멍이 포도청이라 ... 앞으로 보다 더 나은 세상을 만들기 위해서는 자연어 처리 관련 연구가 활발하게 이뤄져야 하고 보다 더 나은 서비스로 사람들의 삶을 더욱 풍요롭게하는 날을 그려봅니다. 발표자료는 [여기](https://songys.github.io/2019LangCon/about/)에서 볼수 있다고 합니다. 